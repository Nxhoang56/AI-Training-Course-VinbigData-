{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "Y2OUp3hR29W9"
   },
   "source": [
    "<img src ='Pic\\logo.jpg' width='500px'>\n",
    "\n",
    "# PHÂN LOẠI COMMNET (TOXIC - NON TOXIC) SỬ DỤNG HỌC MÁY (Text Classification)\n",
    "---\n",
    "Xử lý ngôn ngữ tự nhiên (natural language processing - NLP) là một nhánh của trí tuệ nhân tạo tập trung vào các ứng dụng trên ngôn ngữ của con người. Trong trí tuệ nhân tạo thì xử lý ngôn ngữ tự nhiên là một trong những phần khó nhất vì nó liên quan đến việc phải hiểu ý nghĩa ngôn ngữ-công cụ hoàn hảo nhất của tư duy và giao tiếp. (wikipedia)\n",
    "\n",
    "<img src='Pic/Picture1.jpg'>\n",
    "\n",
    "Tập dữ liệu bao gồm 56700 comment với 2 thuộc tính (class, tweet),\n",
    "\n",
    "\n",
    "1.   Class cho biết comment này thuộc lớp nào:\n",
    "    *   0 : Không độc - Non Toxic\n",
    "    *   1 : Độc - Toxic \n",
    "2. Tweet tập hợp các commnet, dữ liệu này chưa được xử lý.\n",
    "\n",
    "\n",
    "**CÁC VẤN ĐỀ GIẢI QUYẾT TRONG PROJECT:**\n",
    "* Nghiên cứu các phương pháp tiền xử lý dữ liệu văn bản\n",
    "* Nghiên cứu các phương pháp trích chọn đặc trưng của dữ liệu văn bản\n",
    "* Nghiên cứu sử dụng các thuật toán học máy phân lớp văn bản (Toxic - Non Toxic)\n",
    "\n",
    "<img src='Pic/Picture2.png'>\n",
    "\n",
    "---\n",
    "*Copyright: Đặng Văn Nam - FIT.HUMG  - AIAcademy @2021*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk as nltk\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lKK4HIas7F4w"
   },
   "source": [
    "## I. TẢI TẬP DỮ LIỆU DATA_NLP VÀ QUAN SÁT TẬP DỮ LIỆU\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "KUwc65m76cUI",
    "outputId": "0bc5bbbe-7096-4b12-f992-abd2e8db6289"
   },
   "outputs": [],
   "source": [
    "#Đọc file dữ liệu Data_NLP.csv vào biến data\n",
    "path='Data/Data_NLP.csv'\n",
    "data = pd.read_csv(path)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "fDT8NVvmiiLC",
    "outputId": "a1008912-b77d-4d22-c885-aac8ab35cbfc"
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "GWEMPKiQiqxY",
    "outputId": "2f555f53-9d68-46fe-ac78-a23669391431"
   },
   "outputs": [],
   "source": [
    "print(data.loc[[56600],['tweet']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "colab_type": "code",
    "id": "9BZj5GlxhDR_",
    "outputId": "7efee8f9-9623-4078-c132-d460b67e0d41"
   },
   "outputs": [],
   "source": [
    "#Kiểm tra dữ liệu missing trong tập: --> Không có dữ liệu missing\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UGmKfPWf8yRu"
   },
   "source": [
    "## II.TIỀN XỬ LÝ DỮ LIỆU VĂN BẢN\n",
    "---\n",
    "\n",
    "Sử dụng thư viện Natural Language Tool Kit (NLTK) thực hiện xử lý các comment\n",
    "\n",
    "NLTK là một bộ công cụ dành riêng cho Natural Language Processing và được tích hợp vào Python. Nó đang ngày càng hoàn thiện và tích hợp các công cụ mới bởi hàng nghìn lập trình viên và cộng tác viên trên khắp thế giới. NLTK bao gồm những thư viện hàm, các công cụ phân tích, các corpus, wordnet, …giúp đơn giản hóa, tiết kiệm thời gian và công sức cho các lập trình viên. Python kết hợp với NLTK là bộ công cụ hữu hiệu và mạnh mẽ nhất dành cho Natural Language Processing.\n",
    "\n",
    "http://www.nltk.org/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "QA_5uA_K88yQ",
    "outputId": "13193c55-844c-4e1d-d0e9-5bb90b5be8ab"
   },
   "outputs": [],
   "source": [
    "#Khai báo sử dụng thư viện NLTK\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I2f4esC0zAce"
   },
   "source": [
    "### 2.1) Làm sạch dữ liệu\n",
    "----\n",
    "Mục đích bước này là loại bỏ noise trong data của bạn. Đa phần noise là các thẻ HTML, JavaScript, và đương nhiên nếu cứ để noise để tiến hành xử lý sẽ dẫn đến kết quả xử lý không tốt.\n",
    "\n",
    "Thư viện re — Regular expression operations: thao tác với biểu thức chính quy\n",
    "\n",
    "https://regex101.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wdZSsvTX_-K8"
   },
   "outputs": [],
   "source": [
    "#Thư viện re — Regular expression operations: thao tác với biểu thức chính quy\n",
    "import re\n",
    "\n",
    "#hàm decontracted thực hiện chuyển đổi các phần viết tắt thành câu đầy đủ\n",
    "def decontracted(st):\n",
    "    # specific\n",
    "    st = re.sub(r\"won\\'t\", \"will not\", st)\n",
    "    st = re.sub(r\"can\\'t\", \"can not\", st)\n",
    "    # general\n",
    "    st = re.sub(r\"n\\'t\", \" not\", st)\n",
    "    st = re.sub(r\"\\'re\", \" are\", st)\n",
    "    st = re.sub(r\"\\'s\", \" is\", st)\n",
    "    st = re.sub(r\"\\'d\", \" would\", st)\n",
    "    st = re.sub(r\"\\'ll\", \" will\", st)\n",
    "    st = re.sub(r\"\\'ve\", \" have\", st)\n",
    "    st = re.sub(r\"\\'m\", \" am\", st)\n",
    "    return st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0GoT4TzvUzP1"
   },
   "outputs": [],
   "source": [
    " #hàm clear_link thực hiện loại bỏ liên kết (link), địa chỉ email trong câu\n",
    "def clear_link(st):\n",
    "    #Remove links/email\n",
    "    word = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', \n",
    "                '', st, flags=re.MULTILINE)\n",
    "    word = re.sub(r'(@[^\\s]*)', \"\", word)\n",
    "    #word = re.sub('[\\W]', ' ', st)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tdlyIHJWXOJ"
   },
   "outputs": [],
   "source": [
    "#Hàm clear_punctuation thực hiện loai bỏ các dấu câu, ký tự đặc biệt trong chuỗi\n",
    "def clear_punctuation(st):\n",
    "    word = re.sub(r'[^\\w\\s]', '',st)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0QqSC2daXV41"
   },
   "outputs": [],
   "source": [
    "#Hàm clear_special loại bỏ các ký tự chỉ để lại các ký tự chữ a-z, A-Z\n",
    "def clear_special(st):\n",
    "    word = re.sub('[^a-zA-Z]', ' ', st)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hp_haEeNXbn8"
   },
   "outputs": [],
   "source": [
    "#Hàm clear_noise kết hợp sử dụng các hàm ở trên để xử lý chuỗi\n",
    "\n",
    "def clear_noise(word):\n",
    "    word = word.lower()         # chuyển toàn bộ sang chữ thường để xử lý\n",
    "    word = decontracted(word)\n",
    "    word = clear_link(word)\n",
    "    word = clear_punctuation(word)\n",
    "    word = clear_special(word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "FN33R8CIDWvO",
    "outputId": "19876ac0-999e-4133-a362-7559bf474f43"
   },
   "outputs": [],
   "source": [
    "#Kiểm tra clear_noise:\n",
    "text=\"\"\"\"SORRY JB FAN &#65292;REALLY JUSTIN COME BACK\". \n",
    "That is the top comment on a Justin Timberlake video with 10 million views. \n",
    "Son of a fucking bitch! I'm looking forward this video. Have you been seen it yet. \n",
    "fanjustin@gmail.com; https://allflim.com\"\"\"\n",
    "#text = \"\"\"I won't give up @nam to learn English 123; Email: dangnam1985@gmail.com; web: http://dantri.com.vn\"\"\"\n",
    "#print(decontracted(text))\n",
    "#print(clear_link(text))\n",
    "#print(clear_punctuation(text))\n",
    "#print(clear_special(text))\n",
    "print(\"Dữ liệu ban đầu: \\n\", text)\n",
    "print('-----------------------------------------------')\n",
    "text1 = clear_noise(text)\n",
    "print(\"Dữ liệu sau khi làm sạch: \\n\", clear_noise(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDwf8ElpbloG"
   },
   "source": [
    "### 2.2) Loại bỏ stopword\n",
    "---\n",
    "\n",
    "StopWords là những từ xuất hiện nhiều trong ngôn ngữ tự nhiên, tuy nhiên lại không mang nhiều ý nghĩa. Ở tiếng việt StopWords là những từ như: để, này, kia... Tiếng anh là những từ như: is, that, this... Tham khảo thêm tại danh sách stopwords trong tiếng việt\n",
    "\n",
    "Có rất nhiều cách để loại bỏ StopWords nhưng có 2 cách chính là:\n",
    "\n",
    "* Dùng từ điển\n",
    "\n",
    "* Dựa theo tần suất xuất hiện của từ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "Ji4XxzOipK1a",
    "outputId": "47b305ea-f1f1-4693-b146-4766b92c29d5"
   },
   "outputs": [],
   "source": [
    "#stopwords là những từ xuất hiện nhiều trong văn bản, nhưng ko có ý nghĩa \n",
    "#Load danh sách Stopword trong tiếng anh\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "vMavhhICna57",
    "outputId": "9c066ae6-a12e-4ccf-fd6a-af08188fcada",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Hiển thị danh sách các stopwords trong tiếng anh\n",
    "print('Hiển thị danh sách các stopwords trong tiếng anh:\\n',stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "427fDeFnnqMI"
   },
   "outputs": [],
   "source": [
    "#hàm clear_stopwords lọai bỏ các từ stopword trong câu\n",
    "def clear_stopwords(st):\n",
    "    word = \" \".join(st for st in st.split() if st not in stop)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "SWJrRRWMoUCl",
    "outputId": "05e57fad-2d3c-4bf3-c919-5da320731084"
   },
   "outputs": [],
   "source": [
    "print('Dữ liệu ban đầu: \\n',text1)\n",
    "print('-------------------------------------')\n",
    "text2 = clear_stopwords(text1)\n",
    "print('Dữ liệu loại bỏ stopwords:\\n',text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_ue2I8nh1VH"
   },
   "source": [
    "### 2.3) Chuẩn hóa từ (Stemming và Lemmatization)\n",
    "---\n",
    "Trong quá trình xử lý ngôn ngữ tự nhiên, chúng ta sẽ có nhu cầu so sánh các từ (token) với nhau. Việc so sánh này tưởng chừng như đơn giản là lấy 2 chuỗi ký tự và dùng phép “==” để kiểm tra, nhưng thực tế thì không phải là như vậy. Đối với một số ngôn ngữ, tiêu biểu là tiếng Anh, mỗi từ có thể có nhiều biến thể khác nhau. Điều này làm cho việc so sánh giữa các từ là không thể mặc dù về mặc ý nghĩa cơ bản là như nhau. Ví dụ các từ “walks“, “walking“, “walked” đều là các biến thể của từ “walk” và đều mang ý nghĩa là “đi bộ”. Vậy làm sao để so sánh các từ như thế với nhau? Lemmatization và Stemming chính là 2 kỹ thuật thường được dùng cho việc này.\n",
    "\n",
    "Stemming là kỹ thuật dùng để biến đổi 1 từ về dạng gốc (được gọi là stem hoặc root form) bằng cách cực kỳ đơn giản là loại bỏ 1 số ký tự nằm ở cuối từ mà nó nghĩ rằng là biến thể của từ. Ví dụ như chúng ta thấy các từ như walked, walking, walks chỉ khác nhau là ở những ký tự cuối cùng, bằng cách bỏ đi các hậu tố –ed, –ing hoặc –s, chúng ta sẽ được từ nguyên gốc là walk. Người ta gọi các bộ xử lý stemming là Stemmer.\n",
    "\n",
    "Bởi vì nguyên tắc hoạt động của stemmer rất là đơn giản như vậy cho nên tốc độ xử lý của nó rất là nhanh, và kết quả stem đôi khi không được như chúng ta mong muốn. Chẳng hạn như từ goes sẽ được stem thành từ goe (bỏ chữ s cuối từ) trong khi đó stem của từ go vẫn là go, kết quả là 2 từ “goes” và “go” sau khi được stem thì vẫn không giống nhau. Một nhược điểm khác là nếu các từ dạng bất quy tắt như went hay spoke thì stemmer sẽ không thể đưa các từ này về dạng gốc là go hay speak.\n",
    "\n",
    "Tuy có các nhược điểm như trên nhưng trong thực tế Stemming vẫn được sử dụng khá phổ biến trong NLP vì nó có tốc độ xử lý nhanh và kết quả cuối cùng nhìn chung không hề tệ khi so với Lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4a7qdK1KjHpA"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wn = WordNetLemmatizer()\n",
    "import string\n",
    "my_sw = ['rt', 'ht', 'fb', 'amp', 'gt']\n",
    "\n",
    "def black_txt(token):\n",
    "  if token == 'u':\n",
    "    token = 'you'\n",
    "  return  token not in stop and token not in list(string.punctuation) and token not in my_sw\n",
    "\n",
    "\n",
    "def fun_stemlem(word):\n",
    "  list_word_clean = []\n",
    "  for w1 in word.split(\" \"):\n",
    "    if  black_txt(w1.lower()):\n",
    "      word_lemma =  wn.lemmatize(w1,  pos=\"v\")\n",
    "      list_word_clean.append(word_lemma)\n",
    "\n",
    "  #Cleaning, lowering and remove whitespaces\n",
    "  word = \" \".join(list_word_clean)\n",
    "  return word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dữ liệu ban đầu: \\n',text2)\n",
    "print('-------------------------------------')\n",
    "text3 = fun_stemlem(text2)\n",
    "print('Dữ liệu sau khi đã chuẩn hóa:\\n',text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "f3spT140lfz-",
    "outputId": "843bef10-30b0-4f66-a4fa-3cd1c3d1bab8"
   },
   "outputs": [],
   "source": [
    "#Ví dụ một comment trong data:\n",
    "st1 = data.iloc[2000]['tweet']\n",
    "st1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ozS-E8nojxD2",
    "outputId": "433896df-e455-4519-d4b8-2e04d624635a"
   },
   "outputs": [],
   "source": [
    "print('1.----------Chuỗi ban đầu------------------------:\\n ', st1)\n",
    "sta = clear_noise(st1)\n",
    "print('2.----------Chuỗi loại bỏ nhiễu:-----------------:\\n',sta)\n",
    "stb = clear_stopwords(sta)\n",
    "print('3.----------Chuỗi loại bỏ stopwords--------------:\\n',stb)\n",
    "stc = fun_stemlem(stb)\n",
    "print('4.--Chuỗi loại xử lý Stemming và Lemmatization---:\\n',stc)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0emrSyAVnRXL"
   },
   "source": [
    "### 2.4) Tiền Xử lý toàn bộ tập dữ liệu với các hàm đã xây dựng\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "suNXjcecnYuY"
   },
   "outputs": [],
   "source": [
    "#Xây dựng hàm prepare_data: để thực hiện tiền xử lý dữ liệu\n",
    "def prepare_data(word):\n",
    "    word = clear_noise(word)        #Loại bỏ nhiễu trong các comment\n",
    "    word = clear_stopwords(word)    #Loại bỏ stopword trong các comment\n",
    "    word = fun_stemlem(word)        #Chuẩn hóa comment\n",
    "    return word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "V5GNdQJXnw9u",
    "outputId": "5b963ed3-3d6d-4b97-8a7b-3011b8b3f3b0"
   },
   "outputs": [],
   "source": [
    "#Hiển thị danh sách các từ trước và sau khi tiền xử lý để test\n",
    "for idx in data[30090:30094].index:\n",
    "  print(idx, \n",
    "        '\\n a.Dữ liệu gốc          :', data.iloc[idx]['tweet'],\n",
    "        '\\n b.Dữ liệu sau chuẩn hóa:',prepare_data(data.iloc[idx]['tweet']))\n",
    "  print(\"************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d8fCL6WuCRKE"
   },
   "outputs": [],
   "source": [
    "#Tạo dataframe data_new, bổ sung thêm field: tweet_ok là comment đã được tiền xử lý dữ liệu tương ứng\n",
    "data_new = data.copy()\n",
    "data_new['tweet_ok'] = data['tweet'].apply(lambda x: prepare_data(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "NMM8hRwyCWjd",
    "outputId": "e58f59fa-31ca-4c14-e197-c644518227ed"
   },
   "outputs": [],
   "source": [
    "data_new.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-pcUag9oDwE"
   },
   "source": [
    "### lọc và loại bỏ một số comment rỗng sau chuẩn hóa:\n",
    "---\n",
    "Sau khi thực hiện tiền xử lý, một số comment chỉ còn lại xâu rỗng, cần loại bỏ những xâu này\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lọc những comment sau khi xử lý chỉ còn là khoảng trắng\n",
    "print('Tổng số:',data_new['tweet_ok'].loc[data_new.tweet_ok==r''].count())\n",
    "data_new.loc[data_new.tweet_ok==r'']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chỉ lấy các bản ghi có dữ liệu:\n",
    "data_ok = data_new.loc[data_new.tweet_ok!=r'']\n",
    "data_ok.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Kiểm tra mức độ cân bằng của tập dữ liệu:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "Dr_oamhB7lKg",
    "outputId": "36119260-018b-43b9-c0ce-dea3c9300c18"
   },
   "outputs": [],
   "source": [
    "#Kiểm tra mức độ cân bằng của tập dữ liệu\n",
    "print('Thông kê tập dữ liệu:')\n",
    "print(data_ok.count())\n",
    "print('-----------------------------------')\n",
    "print('Thông kê số lượng comment theo lớp:')\n",
    "x = data_ok['class'].value_counts()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "Fr3-HYPC7xpf",
    "outputId": "c2617795-0d71-47aa-d154-4acf4f7c8269"
   },
   "outputs": [],
   "source": [
    "#Trực quan hóa số liệu\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "label = ['0: Non Toxic', '1: Toxic']\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
    "plt.suptitle('THÔNG KÊ SỐ LƯỢNG COMMENT ĐỘC (1) - KHÔNG ĐỘC (0)')\n",
    "\n",
    "ax[0].pie(x, labels=label, autopct='%.2f%%')\n",
    "ax[1].bar(label,x, color='brown')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V755mg30h9OM"
   },
   "source": [
    "### 2.6) Lưu kết quả đã xử lý ra file\n",
    "---\n",
    "\n",
    "Lưu dữ liệu đã xử lý ra file:\n",
    "* File: data_all.csv (class - tweet - tweet_ok)\n",
    "* File: data_finish.csv (class - tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "YRzweHXRePBz",
    "outputId": "1ffc0bb1-1c75-4c7d-cecf-6a4fb7e3fa7d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_ok.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4qxgrm3JFqiz"
   },
   "outputs": [],
   "source": [
    "#Lưu kết quả sau khi đã xử lý ra file .csv\n",
    "#File Data_all.csv Lưu trữ 3 thuộc tính: class - tweet (gốc) - tweet_ok (đã xử lý)\n",
    "data_ok.to_csv('Data/Data_all.csv',index=None, header=True)\n",
    "\n",
    "#File Data_finish.csv chỉ lưu trữ 2 thuộc tính: class - tweet_ok (đã xử lý)\n",
    "data_ok[['class','tweet_ok']].to_csv('Data/Data_finish.csv',index=None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gW6npz32n7Xj"
   },
   "source": [
    "### 2.7) Phân tách Biến độc - Biến phụ thuộc\n",
    "---\n",
    "* Biến độc lập X: Thuộc tính tweet_ok (nội dung comment sau khi đã được xử lý)\n",
    "* Biến phụ thuộc y: class (cho biết comment đó là Toxic - Non toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hnqhVSMYQYxf"
   },
   "outputs": [],
   "source": [
    "#Tải file dữ liệu sau khi đã tiền xử lý\n",
    "import pandas as pd\n",
    "path='Data/data_finish.csv'\n",
    "data_finish = pd.read_csv(path)\n",
    "data_finish.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Biến độc lập X:\n",
    "X = data_finish.loc[:,'tweet_ok']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Biến phụ thuộc y:\n",
    "y = data_finish.loc[:,'class']\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ueATfcXLno68"
   },
   "source": [
    "### 2.8) Tách dữ liệu thành tập Train - Test\n",
    "---\n",
    "* Tập Train (80%) sử dụng để huấn luyện model\n",
    "* Tập Test (20%) sử dụng để kiểm thử độ chính xác của modoe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "CDhCUeXUJO7C",
    "outputId": "8611a9fd-f90a-44e7-9b8f-d97e96ff959b"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Tách tập dữ liệu thành Train - Test (tỷ lệ: 0.8 - 0.2)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,\n",
    "                                                 test_size=0.2,\n",
    "                                                 random_state=42)\n",
    "print('1.Tập ban đầu:',data_finish.shape)\n",
    "print('-------------------------------')\n",
    "print('a.Tập Train: ', X_train.shape)\n",
    "print('b.Tập Test: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZVO8i-IPmT3"
   },
   "source": [
    "## III) Embedding TẬP DỮ LIỆU ĐÃ XỬ LÝ\n",
    "---\n",
    "Khi huấn luyện mô hình máy học để xử lý ngôn ngữ tự nhiên, thì chắc chắn dữ liệu bạn có ở dạng chữ viết, sự thật là bạn không thể đem trực tiếp dữ liệu chữ viết thô vào để huấn luyện mô hình máy học ngay được, bởi vì các mô hình máy học chỉ làm việc được trên những con số, hay chính xác hơn là tính toán trên các ma trận, véc-tơ số.\n",
    "\n",
    "Điều này dẫn đến việc bạn phải nghĩ cách làm thế nào đó để chuyển dữ liệu chữ viết thô thành dữ liệu số thực, sau đó mới có thể đưa dữ liệu số thực này vào các mô hình học được, có rất nhiều kĩ thuật để làm việc này và gọi chung chúng là kĩ thuật Embedding.\n",
    "\n",
    "Đó chỉ mới là ý tưởng của kĩ thuật Embedding, còn việc chuyển từ không gian này sang một không gian vec-tơ khác bạn không thể làm tùy tiện được mà phải đảm bảo:\n",
    "\n",
    "* Không gian véc-tơ mới (hay véc-tơ số thực) phải thể hiện được bản chất của tập dữ liệu ban đầu (dữ liệu chữ viết).\n",
    "* Cực tiểu hóa lượng mất mát thông tin xảy ra khi bạn chuyển sang không gian mới.\n",
    "\n",
    "Một số kĩ thuật Embedding được sử dụng phổ biến như mạng Neural Network, PCA (Principal Component Analysis) gọi là kĩ thuật phân tích thành phần chính, TF-IDF, Bag of Word, Encoder-Decoder sử dụng trong RNN (Recurrent Neural Network) hoặc LSTM (Long-Short Term Memory), ...v.v.\n",
    "\n",
    "Một số thư viện sử dụng kĩ thuật Embedding như sklearn, Word2vec, FastText, ...v.v."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uvuIS6qQJVRZ"
   },
   "source": [
    "### 3.1) TF-IDF\n",
    "----\n",
    "Viết tắt của thuật ngữ tiếng Anh term frequency – inverse document frequency,tf-idf là trọng số của một từ trong văn bản thu được qua thống kê thể hiện mức độ quan trọng của từ này trong một văn bản, mà bản thân văn bản đang xét nằm trong một tập hợp các văn bản.\n",
    "\n",
    "**Các tính trọng số TF-IDF:**\n",
    "* Tf- term frequency : dùng để ước lượng tần xuất xuất hiện của từ trong văn bản. Tuy nhiên với mỗi văn bản thì có độ dài khác nhau, vì thế số lần xuất hiện của từ có thể nhiều hơn . Vì vậy số lần xuất hiện của từ sẽ được chia độ dài của văn bản (tổng số từ trong văn bản đó)\n",
    "\n",
    "    * TF(t, d) = (số lần từ t xuất hiện trong văn bản d) / (tổng số từ trong văn bản d)\n",
    "\n",
    "* IDF- Inverse Document Frequency: dùng để ước lượng mức độ quan trọng của từ đó như thế nào . Khi tính tần số xuất hiện tf thì các từ đều được coi là quan trọng như nhau. Tuy nhiên có một số từ thường được được sử dụng nhiều nhưng không quan trọng để thể hiện ý nghĩa của đoạn văn. Vì vậy ta cần giảm đi mức độ quan trọng của những từ đó bằng cách sử dụng IDF :\n",
    "\n",
    "    * IDF(t, D) = log_e( Tổng số văn bản trong tập mẫu D/ Số văn bản có chứa từ t )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xu-08BJvKDn5"
   },
   "outputs": [],
   "source": [
    "# Tính TF-IDF cho tập dữ liệu\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.feature_extraction.text as text\n",
    "\n",
    "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "vector = TfidfVectorizer(analyzer='word', max_features=15000, stop_words = 'english')\n",
    "\n",
    "vector.fit(data_finish['tweet_ok'])\n",
    "X_train_tfidf = vector.transform(X_train)\n",
    "X_test_tfidf = vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('1.Train:',X_train_tfidf.shape)\n",
    "print('2.Test:',X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "lFXbvNWJKEmO",
    "outputId": "2380edfd-c2f4-46bd-8e14-48a342a92bde"
   },
   "outputs": [],
   "source": [
    "#(train_x[1])\n",
    "print(X_train.iloc[10])\n",
    "print(X_train_tfidf[10].data)\n",
    "print(X_train_tfidf[10].data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZmzynBFI1Z5"
   },
   "source": [
    "## IV. MACHINE LEARNING CHO PHÂN LỚP VĂN BẢN\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thuật toán Naive Bayes\n",
    "---\n",
    "* Naive Bayes Classifiers (NBC) thường được sử dụng trong các bài toán Text Classification\n",
    "* NBC có thời gian training và test rất nhanh. Điều này có được là do giả sử về tính độc lập giữa các thành phần, nếu biết class.\n",
    "\n",
    "* Nếu giả sử về tính độc lập được thoả mãn (dựa vào bản chất của dữ liệu), NBC được cho là cho kết quả tốt hơn so với SVM và logistic regression khi có ít dữ liệu training.\n",
    "\n",
    "* NBC có thể hoạt động với các feature vector mà một phần là liên tục (sử dụng Gaussian Naive Bayes), phần còn lại ở dạng rời rạc (sử dụng Multinomial hoặc Bernoulli).\n",
    "\n",
    "* Khi sử dụng Multinomial Naive Bayes, Laplace smoothing thường được sử dụng để tránh trường hợp 1 thành phần trong test data chưa xuất hiện ở training data.\n",
    "\n",
    "\n",
    "#### Sử dụng MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sử dụng mô hình Naive Bayes với TF-IDF\n",
    "from sklearn import naive_bayes as nb\n",
    "\n",
    "MultiNB = nb.MultinomialNB(alpha=0.75)\n",
    "\n",
    "#huấn luyện mô hình với tập huấn luyện Train\n",
    "MultiNB.fit(X_train_tfidf,y_train)\n",
    "\n",
    "#Đánh giá độ chính xác của mô hình trên tập huấn luyện\n",
    "acc_MultiNB = round(MultiNB.score(X_train_tfidf, y_train) * 100, 2)\n",
    "\n",
    "print('Độ chính xác của mô hình trên tập huấn luyện: ', acc_MultiNB, '%')\n",
    "print(MultiNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = MultiNB.predict(X_test_tfidf)\n",
    "\n",
    "acc1 = round(accuracy_score(y_test, y_pred)*100, 2)\n",
    "print('1.Độ chính xác của mô hình trên tập Test: ', acc1, '%')\n",
    "\n",
    "acc2 = accuracy_score(y_test, y_pred, normalize=False)\n",
    "print('2.Tổng số mẫu dự đoán đúng:', acc2, ' /', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trực quan hóa ma trận Confusion matrix:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1, keepdims = True)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Ground Truth label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sử dụng ma trận confussion matrix kiểm tra kết quả:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix_NLP = confusion_matrix(y_test,y_pred,)\n",
    "class_names = [0,1]\n",
    "plot_confusion_matrix(cnf_matrix_NLP,\n",
    "                      classes=class_names,\n",
    "                      title='Confusion matrix - NLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ví dụ với một comment mới --> Sử dụng model đã huấn luyện để cho biết comment này là độc (1) hay không độc (0)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_comment= {'Comment':['you are very handsome, i love you!', \n",
    "                         'You look like a dog, i hate you',\n",
    "                         'i see a rose, do you like roses?',\n",
    "                         'she is very ugly',\n",
    "                         ' ']}\n",
    "df_comment = pd.DataFrame(dict_comment)\n",
    "df_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment['Comment_ok']  = df_comment['Comment'].apply(lambda x: prepare_data(x))\n",
    "df_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_comment = df_comment.iloc[:,1]\n",
    "X_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_comment_tfidf = vector.transform(X_comment)\n",
    "X_comment_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_new = MultiNB.predict(X_comment_tfidf)\n",
    "y_pred_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_Detection_Toxic.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
